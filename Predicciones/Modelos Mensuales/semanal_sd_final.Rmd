---
title: "**Evaluación de modelos para la predicción semanal de accidentes viales con Gravedad: Solo Daños**<br>"

author: "*Alexis A. Arenas Bustamante, Juan E. Arroyave Duque, William A. Jovel Tamayo, Manuela Londoño Ocampo.* <br> Curso de Analítica Predictiva <br> Universidad Nacional de Colombia <br> Facultad de Minas <br> Medellín"
date: "Septiembre de 2019"

output:
  html_document:
    theme: cosmo
    highlight: haddock
    number_sections: true
    df_print: paged
    toc: true
    toc_float:
      collapsed: True
---

<style>
body {
text-align: justify}
</style>

# **Librerías Necesarias**

```{r, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr) # data wrangling
library(ranger)
library(caret)
library(corrplot)
library(rsample)     # data splitting 
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
require(MASS)
library(randomForest)
library(tidyverse)
library(ggpubr)
```

Se consideran tres modelos para la predicción semanal de los accidentes viales con solo daños: Regresión lineal, Árbol de regresión y Random Forest. A continuación se presenta la evaluación de cada uno:

# **Dataset propuesto para el modelo semanal de accidentalidad vial : Solo daños**

```{r, echo= FALSE}
semanal_1 <- read.csv("~/Docs/MAESTRIA/ANALITICA PREDICTIVA/Trabajo Final/Datasets/RAPIDM/SEGUNDA_CORRIDA/solo_danos_semanal_train.csv", sep=";")

head(semanal_1)
```

# **Estadísticas Descriptivas**  {.tabset .tabset-fade .tabset-pills}

## **Summary del conjunto de datos**

```{r, echo=FALSE}
summary(semanal_1)
```

## **Histograma de la variable dependiente**

```{r, echo= FALSE}
library(ggplot2)

ggplot(semanal_1, aes(SOLO_DANOS)) + 
  geom_histogram(bins=12, fill = "#9999CC", position = 'identity') + 
  xlab("Accidentes con Solo Daños") + ylab("Frecuencia") +
  theme_classic2()

```

## **Boxplots por mes**

```{r hitogramas, echo=FALSE}

datosSD <- semanal_1
datosSD$ANYO <- as.factor(datosSD$ANYO)

ggplot(datosSD, aes(x=ANYO, y=SOLO_DANOS, fill=ANYO)) + 
  geom_boxplot(alpha=0.3) +
  xlab("AÑO") + ylab("Accidentes con solo daños") +
  theme(legend.position="none") + theme_classic()
```

No se observan valores atípicos.

## **Correlación entre variables consideradas**

```{r correlacion, echo=FALSE}
library(corrplot)
datosSD$ANYO <- as.integer(datosSD$ANYO)
M <- cor(datosSD)
corrplot(M, method = "color")
```

Se puede observar que la variable mas correlacionada con la cantidad de accidentes semanales con solo daños es FESTIVOS. En cada uno de los modelos se identificará la importancia de las variables, de modo que solo se incluyan aquellas que tengan una influencia significativa sobre la variables dependiente. El periodo no hará parte de ningún modelo puesto que solo se usa para la partición de los datos en conjuntos de entrenamiento y validación.

# **Partición de los datos conjuntos de train y test**

Para la validación se usaran los datos de accidentes viales ocurridos en el año 2018.

Datos de Train: Se cuenta 303 observaciones para el entrenamiento de los modelos

```{r, echo= FALSE}
soloD_train <- datosSD[datosSD$ANYO < 5, ]
soloD_test <- datosSD[datosSD$ANYO >= 5, ]

dim(soloD_train)
```

Datos de Test: Se cuenta con 74 observaciones para la validación de los modelos

```{r, echo= FALSE}
dim(soloD_test)
```

Una vez realizada la partición de los datos, se procede con la eliminación de la variable ANYO en ambos datasets.

```{r}
soloD_train <- dplyr :: select(soloD_train, -ANYO)
soloD_test <- dplyr :: select(soloD_test, -ANYO)
```

# **Modelo de predicción considerados** {.tabset .tabset-fade .tabset-pills}

## **Regresión Lineal**

**Entrenamiento del modelo**

```{r}
soloD_train$SEMANA <- as.factor(soloD_train$SEMANA)
regresion_1 <- lm(SOLO_DANOS ~ ., data= soloD_train)
summary(regresion_1)
```

Las variable de SEMANA se incluyó en el modelo de regresión lineal como factor, vemos que la mayoría de variables son significativas para el modelo y según el summary se tiene un buen R-squared, que explica el 88.72 % de la accidentalidad semanal con gravedad de solo daños. De igual manera el valor P del estadístico F también es significativo.

**Métricas de Bondad para regresión lineal**

```{r, echo = FALSE}
# Compute R^2 from true and predicted values
soloD_test$SEMANA <- as.factor(soloD_test$SEMANA)

y_pred_train_sd <- predict(regresion_1, soloD_train)
y_pred_test_sd <- predict(regresion_1, soloD_test)

reg_1 <- data.frame(RMSE_Train = RMSE(y_pred_train_sd, soloD_train$SOLO_DANOS),
                    Rsquared_Train = R2(y_pred_train_sd, soloD_train$SOLO_DANOS),
                    RMSE_Test = RMSE(y_pred_test_sd, soloD_test$SOLO_DANOS),
                    Rsquared_Test = R2(y_pred_test_sd, soloD_test$SOLO_DANOS))
reg_1
```

Comparando las métricas consideradas tanto para el entrenamiento como para la validación del modelo, observamos que en ambos se obtiene un R-squared bueno, superior al 85%. Adicionalmente la variación entre los RMSE es del 0,73 %, que indica un buen ajuste para ambos conjuntos de datos.

**Ajuste de las predicciones vs valores reales**

```{r}
plot(y_pred_test_sd, soloD_test$SOLO_DANOS,las=1, xlab="Predicciones", ylab = "Accidentes con solo daños")
abline(a=1,b=1,lwd=2,col="red")
```

En el gráfico de dispersión se puede observar que el modelo tiene mejor capacidad de predicción para valores de accidentalidad  bajos pero no tanto para valores altos.

## **Árbol Regresor**

**Entrenamiento del modelo**

Se toma el modelo mas simple para el árbol regresor, a partir de este se realizara una búsqueda tipo **grid search**, de los parámetros de restricción óptimos. Se definen ademas funciones para extraer el mínimo error asociado con el valor óptimo de costo de complejidad (CP) de cada modelo y  el valor óptimo CP.

```{r}
set.seed(123)
dt_sd <- rpart(
  formula = SOLO_DANOS ~ .,
  data    = soloD_train,
  method  = "anova"
  )
```

**Representación gráfica del árbol regresor sin tuning**

```{r, echo= FALSE}
rpart.plot(dt_sd)
```

**Ajuste de parámetro de restricción para el árbol regresor**

Se realiza una búsqueda de los parámetros óptimos de minsplit y maxdepth para el árbol regresor: 

En la siguiente gráfica se puede observar el numero optimo de nodos identificado por el modelo que reduce el error asociado. Este sera el punto de partida para definir entre que valores puede variar el parámetro de profundidad del árbol. Vemos que este valor es de 4 divisiones y 5 nodos finales.


```{r}
plotcp(dt_sd)

dt_sd2 <- rpart(
    formula = SOLO_DANOS ~ .,
    data    = soloD_train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

```

El error de validación cruzada asociado es de 0.2519, este valor se puede mejorar en el proceso de tuning

```{r, echo= FALSE}
dt_sd$cptable
```

El siguiente código define una función grid para la búsqueda de diferentes combinaciones de minsplit y maxdepth. También las funciones que extraen el optimo cp y el mínimo error asociado.

```{r}

hyper_grid_sd <- expand.grid(
  minsplit = seq(2, 20, 1),
  maxdepth = seq(2, 30, 1)
)
models_sd <- list()

for (i in 1:nrow(hyper_grid_sd)) {
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid_sd$minsplit[i]
  maxdepth <- hyper_grid_sd$maxdepth[i]

  # train a model and store in the list
  models_sd[[i]] <- rpart(
    formula = SOLO_DANOS ~ .,
    data    = soloD_train,
    method  = "anova",
    control = list(minsplit = minsplit, maxdepth = maxdepth)
    )
}

# function to get optimal cp
get_cp <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}

# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}
```

Agregaremos los resultados de la búsqueda a la grilla de hyper-parámetros y filtraremos los 5 valores con error mínimo.

```{r}
hyper_grid_sd %>%
  mutate(
    cp    = purrr::map_dbl(models_sd, get_cp),
    error = purrr::map_dbl(models_sd, get_min_error)
    ) %>%
  arrange(error) %>%
  top_n(-5, wt = error)
```

Vemos que el error se redujo a 0.198, lo cual indica mejores modelos en comparación con el modelo inicial. Se entrena el árbol regresor con las combinaciones que encabezan los resultados anteriores:

```{r}
optimal_tree_sd <- rpart(
    formula = SOLO_DANOS ~ .,
    data    = soloD_train,
    method  = "anova",
    control = list(minsplit = 8, maxdepth = 5, cp = 0.01)
    )
```

**Métricas de Bondad para el mejor árbol regresor**

```{r}

pred_dt_train = predict(optimal_tree_sd, newdata=soloD_train)
pred_dt_test = predict(optimal_tree_sd, newdata=soloD_test)

dt_sd_op1 <- data.frame(RMSE_Train = RMSE(pred_dt_train, soloD_train$SOLO_DANOS),
                    Rsquared_Train = R2(pred_dt_train, soloD_train$SOLO_DANOS),
                    RMSE_Test = RMSE(pred_dt_test, soloD_test$SOLO_DANOS),
                    Rsquared_Test = R2(pred_dt_test, soloD_test$SOLO_DANOS))

dt_sd_op1
```

**Mejor árbol regresor obtenido**

```{r, echo= FALSE}
library(rpart.plot)
prp(optimal_tree_sd)
```

## **Random Forest**

**Entrenamiento del modelo**

Se toma el modelo mas simple, a partir de este se realizara una búsqueda tipo grid search de los hiper parámetros óptimos, mediante el uso de la librería Range. Se analizaron en total 792 modelos diferentes con variaciones en mtry, mínimo tamaño de muestras en el nodo, y tamaño de la muestra.

```{r}
set.seed(101)
rf_sd = randomForest(SOLO_DANOS ~ ., data = soloD_train)
rf_sd
```

Inicialmente, observamos que la varianza explicada por el modelo es del 85,81 %.

**Este gráfico muestra el error vs. el número de árboles**

```{r, echo= FALSE}
plot(rf_sd)
```

 **Afinamiento del modelo Random Forest**

Se define la siguiente grilla de búsqueda:

```{r}
set.seed(101)
# hyperparameter grid search
hyper_grid_rf <- expand.grid(
  mtry       = seq(1, 9, by = 1),
  node_size  = seq(7, 50, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE   = 0
)

# total number of combinations
#nrow(hyper_grid_rf)
```

Se utiliza la librería Ranger para la búsqueda de los mejores hyper parámetros:

```{r}
for(i in 1:nrow(hyper_grid_rf)) {
  
  # train model
  model <- ranger(
    formula         = SOLO_DANOS ~ ., 
    data            = soloD_train, 
    num.trees       = 500,
    mtry            = hyper_grid_rf$mtry[i],
    min.node.size   = hyper_grid_rf$node_size[i],
    sample.fraction = hyper_grid_rf$sampe_size[i],
    seed            = 101
  )
  
  # add OOB error to grid
  hyper_grid_rf$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_rf %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)
```

Se ajusta el modelo con los mejores hyper parámetros obtenidos:

```{r}
set.seed(101)
mejorRandomforest <- randomForest(SOLO_DANOS ~., data = soloD_train, mtry = 8, 
                                  nodesize = 7, ntree = 500, 
                                  sampe_size= 0.8, importance = TRUE)
mejorRandomforest

```

Con la función importance() se extrae la importancia de las variables.

```{r}
library(tidyverse)
library(ggpubr)
importancia_pred <- as.data.frame(importance(mejorRandomforest, scale = TRUE))
importancia_pred <- rownames_to_column(importancia_pred, var = "variable")
p1 <- ggplot(data = importancia_pred, aes(x = reorder(variable, `%IncMSE`),
                                          y = `%IncMSE`,
                                          fill = `%IncMSE`)) +
    labs(x = "variable", title = "Reducción de MSE") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")

p2 <- ggplot(data = importancia_pred, aes(x = reorder(variable, IncNodePurity),
                                          y = IncNodePurity,
                                          fill = IncNodePurity)) +
    labs(x = "variable", title = "Reducción de pureza") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")
ggarrange(p1, p2)
```

**Métricas de bondad para modelo de Random Forest**

```{r}
pred_randomForest_t <- predict(mejorRandomforest, soloD_train)
pred_randomForest <- predict(mejorRandomforest, soloD_test)

dt_rf_1 <- data.frame(RMSE_Train = RMSE(pred_randomForest_t, soloD_train$SOLO_DANOS),
                    Rsquared_Train = R2(pred_randomForest_t, soloD_train$SOLO_DANOS),
                    RMSE_Test = RMSE(pred_randomForest, soloD_test$SOLO_DANOS),
                    Rsquared_Test = R2(pred_randomForest, soloD_test$SOLO_DANOS))


dt_rf_1

```

Observamos que los R-squared para entrenamiento y validación son mas altos en comparación con los modelos de regresión lineal y Árbol regresor. El RMSE presenta una variación aproximada de 21 %, a pesar de que no es la menor variación de los modelos expuestos, la variación con respecto al Rsquared es muy baja y por tal motivo se elige al RandomForest como mejor modelo.


